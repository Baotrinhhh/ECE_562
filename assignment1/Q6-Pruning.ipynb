{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SblI4JHed_gb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# this mounts your Google Drive to the Colab VM.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m, force_remount\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# enter the foldername in your Drive where you have saved the unzipped\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# assignment folder, e.g. 'ece697ls/assignments/assignment3/'\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# this mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'ece697ls/assignments/assignment3/'\n",
    "FOLDERNAME = None\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDCKl_T8d_gg",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Pruning\n",
    "\n",
    "After learning, neural networks have modified and learned a set of parameters to perform our classification task. However, such parameters are costly to maintain and do not hold the same importance.\n",
    "\n",
    "Wouldn't it be great could optimize our resource usage by dropping less important values ? This is where pruning comes into play.\n",
    "\n",
    "Pruning is a technique that cuts off parameters/structures from a model to increase sparcity and decrease overall model size, similar to cutting leafs or branches from bushes and trees. This process can lead to smaller memory consumption with minimal accuracy reduction. Moreover, pruning the network may also provide a speedup since there will be less operations being performed.\n",
    "\n",
    "The pruning process can be performed during the end of an epoch of training or after training is complete. Experimenting to find out which way works the best is part of the fun !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P242B5PEtU6N"
   },
   "outputs": [],
   "source": [
    "from ece662.pruning_helper import test_model, load_model\n",
    "from ece662.data_utils import get_CINIC10_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76IT2Z-P7aYm"
   },
   "source": [
    "Below we will load a pre-trained model for you to work on. If you prefer, you can save your own model from the previous Tensorflow/Pytorch task and load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jKM5JRLtuVbJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ece662\\models\\torch.model\n",
      "Model loaded successfully!\n",
      "Test Acc: 0.5051\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and test data (running locally, not on Colab)\n",
    "\n",
    "data = get_CINIC10_data()\n",
    "mode = 'torch'  # torch or tensorflow\n",
    "\n",
    "test_data = [data['X_test'], data['y_test']]\n",
    "\n",
    "# Use local path instead of Google Drive path\n",
    "path = os.path.join('ece662', 'models', f'{mode}.model')\n",
    "print(f\"Loading model from: {path}\")\n",
    "\n",
    "model = load_model(path, mode=mode)\n",
    "print(\"Model loaded successfully!\")\n",
    "test_model(model, test_data, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZOsPryFd_gi",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Unstructured Pruning\n",
    "\n",
    "Unstructured Pruning is usually related to the pruning of weights in neural networks. The general idea is to select a set of weights according to a policy and setting them up to zero. \n",
    "\n",
    "Common policies are random weight selection or selecting the smallers weights. \n",
    "Unstructured Pruning can be performed in one or multiple layers within the same network.\n",
    "\n",
    "Altough in theory Unstructured Pruning should decrease the number of operations performed during execution there should be explicit support within the framework or hardware to bypass such operations, otherwise it will just operated over zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjxdSF2xmCei"
   },
   "source": [
    "### Perform Pruning\n",
    "\n",
    "Using the model trained in the previous step using pytorch, perform unstructured pruning in the weights of the model by removing x% of the smallest weights. \n",
    "\n",
    "*   Increment global pruning by 10% until reaching total of 80% pruned weights\n",
    "*   Perform inference at the end of each pruning and observe the impact into the accuracy.\n",
    "\n",
    "\n",
    "Note: The percentages are related to the entire model, not per layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KaGWTxBcmkzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Unstructured Pruning Experiment\n",
      "==================================================\n",
      "Original Model Performance:\n",
      "Test Acc: 0.5089\n",
      "\n",
      "Pruning 10% of weights globally...\n",
      "  Actual sparsity achieved: 10.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.5046\n",
      "------------------------------\n",
      "Pruning 20% of weights globally...\n",
      "  Actual sparsity achieved: 20.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.5052\n",
      "------------------------------\n",
      "Pruning 30% of weights globally...\n",
      "  Actual sparsity achieved: 30.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.5026\n",
      "------------------------------\n",
      "Pruning 40% of weights globally...\n",
      "  Actual sparsity achieved: 40.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.5030\n",
      "------------------------------\n",
      "Pruning 50% of weights globally...\n",
      "  Actual sparsity achieved: 50.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.4828\n",
      "------------------------------\n",
      "Pruning 60% of weights globally...\n",
      "  Actual sparsity achieved: 60.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.4906\n",
      "------------------------------\n",
      "Pruning 70% of weights globally...\n",
      "  Actual sparsity achieved: 70.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.4721\n",
      "------------------------------\n",
      "Pruning 80% of weights globally...\n",
      "  Actual sparsity achieved: 80.00%\n",
      "  Performance after pruning:\n",
      "Test Acc: 0.4093\n",
      "------------------------------\n",
      "\n",
      "Pruning Results Summary:\n",
      "Target Pruning % | Actual Sparsity %\n",
      "-----------------------------------\n",
      "         10%      |       10.00%\n",
      "         20%      |       20.00%\n",
      "         30%      |       30.00%\n",
      "         40%      |       40.00%\n",
      "         50%      |       50.00%\n",
      "         60%      |       60.00%\n",
      "         70%      |       70.00%\n",
      "         80%      |       80.00%\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO: Perform unstructured Pruning over the trained model using 3 different  \n",
    "# prunning percentages.                                \n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "\n",
    "# Create a copy of the original model to preserve it\n",
    "original_model = copy.deepcopy(model)\n",
    "\n",
    "# Define pruning percentages to test (10% increments up to 80%)\n",
    "pruning_percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "print(\"Starting Unstructured Pruning Experiment\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test original model accuracy first\n",
    "print(f\"Original Model Performance:\")\n",
    "test_model(model, test_data, mode=mode)\n",
    "print()\n",
    "\n",
    "# Store results for analysis\n",
    "results = []\n",
    "\n",
    "for prune_amount in pruning_percentages:\n",
    "    print(f\"Pruning {prune_amount*100:.0f}% of weights globally...\")\n",
    "    \n",
    "    # Reset model to original state\n",
    "    model = copy.deepcopy(original_model)\n",
    "    \n",
    "    # Get all parameters to be pruned (conv and linear layers)\n",
    "    parameters_to_prune = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    # Apply global unstructured pruning (removes smallest weights across all layers)\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=prune_amount,\n",
    "    )\n",
    "    \n",
    "    # Make pruning permanent (remove masks and actually set weights to zero)\n",
    "    for module, param_name in parameters_to_prune:\n",
    "        prune.remove(module, param_name)\n",
    "    \n",
    "    # Calculate sparsity (percentage of zero weights)\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total_params += param.numel()\n",
    "            zero_params += (param == 0).sum().item()\n",
    "    \n",
    "    actual_sparsity = zero_params / total_params\n",
    "    \n",
    "    print(f\"  Actual sparsity achieved: {actual_sparsity*100:.2f}%\")\n",
    "    print(f\"  Performance after pruning:\")\n",
    "    \n",
    "    # Test pruned model\n",
    "    test_model(model, test_data, mode=mode)\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'target_prune': prune_amount * 100,\n",
    "        'actual_sparsity': actual_sparsity * 100\n",
    "    })\n",
    "\n",
    "print(\"\\nPruning Results Summary:\")\n",
    "print(\"Target Pruning % | Actual Sparsity %\")\n",
    "print(\"-\" * 35)\n",
    "for result in results:\n",
    "    print(f\"     {result['target_prune']:6.0f}%      |      {result['actual_sparsity']:6.2f}%\")\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zH6e3pCMm2in"
   },
   "source": [
    "## Inline Question 1:\n",
    "\n",
    "What happened with the accuracy as the % of pruning increased ?\n",
    "Why was that the case?\n",
    "\n",
    "\n",
    "## Answer: \n",
    "\n",
    "As the percentage of pruning increased, the model's accuracy generally decreased, but not uniformly:\n",
    "\n",
    "**Observed Results:**\n",
    "- Original Model: 50.89% accuracy\n",
    "- 10%-40% pruning: Minimal accuracy loss (50.46% - 50.30%), showing the model is robust to removing small weights\n",
    "- 50%-60% pruning: More noticeable degradation (48.28% - 49.06%)\n",
    "- 70%-80% pruning: Significant accuracy drop (47.21% - 40.93%)\n",
    "\n",
    "**Why this happened:**\n",
    "1. **Redundancy in Neural Networks**: At low pruning percentages (10%-40%), the smallest weights contribute minimally to the model's predictive power. Removing them has little impact because the network has learned redundant representations.\n",
    "\n",
    "2. **Weight Magnitude Correlation**: The L1 unstructured pruning removes weights with smallest absolute values first. These small weights often represent less important connections, so their removal doesn't significantly hurt performance initially.\n",
    "\n",
    "3. **Critical Threshold**: Beyond 50% pruning, we start removing weights that are more critical for the network's function. The network loses important pathways for information flow, leading to degraded performance.\n",
    "\n",
    "4. **Network Capacity**: At very high pruning levels (70%-80%), the network loses too much of its representational capacity, causing substantial accuracy drops as essential learned features are lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN5CMlgUd_gj",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Structured Pruning\n",
    "\n",
    "Structured Pruning consists of removing a bigger chunk of the network parameters at the same time. Instead of removing only a few weights, it is commonplace to remove entire neurons. \n",
    "\n",
    "For example, in Convolutional Layers, removing filters can be beneficial to improve performance as it greatly decreases the amount of computation performed. However, some of these changes may affect output dimensions which may be carried over to other parts of the network. Therefore, when performing structured pruning one must always be aware of which parameters are going to be affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAr9SLo25Gyx"
   },
   "source": [
    "Using the previously trained model in the CINIC-10, perform Structured Prunning only in the Convolution layers of the DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JFPiHtBohTus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Structured Pruning Experiment on Convolutional Layers\n",
      "============================================================\n",
      "Original Model Performance:\n",
      "Test Acc: 0.5080\n",
      "\n",
      "Model Architecture:\n",
      "  conv1: Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  conv2: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "\n",
      "Structured Pruning: Removing 25% of filters from Conv layers...\n",
      "  Found 2 convolutional layers to prune\n",
      "  Structured sparsity achieved: 25.00% (filters removed)\n",
      "  Performance after structured pruning:\n",
      "Test Acc: 0.5080\n",
      "\n",
      "Model Architecture:\n",
      "  conv1: Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  conv2: Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "\n",
      "Structured Pruning: Removing 25% of filters from Conv layers...\n",
      "  Found 2 convolutional layers to prune\n",
      "  Structured sparsity achieved: 25.00% (filters removed)\n",
      "  Performance after structured pruning:\n",
      "Test Acc: 0.1866\n",
      "----------------------------------------\n",
      "Structured Pruning: Removing 50% of filters from Conv layers...\n",
      "  Found 2 convolutional layers to prune\n",
      "  Structured sparsity achieved: 50.00% (filters removed)\n",
      "  Performance after structured pruning:\n",
      "Test Acc: 0.1866\n",
      "----------------------------------------\n",
      "Structured Pruning: Removing 50% of filters from Conv layers...\n",
      "  Found 2 convolutional layers to prune\n",
      "  Structured sparsity achieved: 50.00% (filters removed)\n",
      "  Performance after structured pruning:\n",
      "Test Acc: 0.1961\n",
      "----------------------------------------\n",
      "Structured Pruning: Removing 75% of filters from Conv layers...\n",
      "  Found 2 convolutional layers to prune\n",
      "  Structured sparsity achieved: 75.00% (filters removed)\n",
      "  Performance after structured pruning:\n",
      "Test Acc: 0.1961\n",
      "----------------------------------------\n",
      "Structured Pruning: Removing 75% of filters from Conv layers...\n",
      "  Found 2 convolutional layers to prune\n",
      "  Structured sparsity achieved: 75.00% (filters removed)\n",
      "  Performance after structured pruning:\n",
      "Test Acc: 0.2155\n",
      "----------------------------------------\n",
      "\n",
      "Structured Pruning Results Summary:\n",
      "Target Pruning % | Structured Sparsity % (Filters)\n",
      "--------------------------------------------------\n",
      "         25%      |         25.00%\n",
      "         50%      |         50.00%\n",
      "         75%      |         75.00%\n",
      "Test Acc: 0.2155\n",
      "----------------------------------------\n",
      "\n",
      "Structured Pruning Results Summary:\n",
      "Target Pruning % | Structured Sparsity % (Filters)\n",
      "--------------------------------------------------\n",
      "         25%      |         25.00%\n",
      "         50%      |         50.00%\n",
      "         75%      |         75.00%\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO: Perform unstructured Pruning over the trained model using 3 different  \n",
    "# prunning percentages.                                \n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "import copy\n",
    "\n",
    "# Reset to original model for structured pruning\n",
    "model = copy.deepcopy(original_model)\n",
    "\n",
    "print(\"Starting Structured Pruning Experiment on Convolutional Layers\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test original model accuracy first\n",
    "print(\"Original Model Performance:\")\n",
    "test_model(model, test_data, mode=mode)\n",
    "print()\n",
    "\n",
    "# Print model architecture to understand conv layers\n",
    "print(\"Model Architecture:\")\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        print(f\"  {name}: {module}\")\n",
    "print()\n",
    "\n",
    "# Define structured pruning percentages for conv layers\n",
    "structured_pruning_percentages = [0.25, 0.50, 0.75]  # 25%, 50%, 75%\n",
    "\n",
    "structured_results = []\n",
    "\n",
    "for prune_amount in structured_pruning_percentages:\n",
    "    print(f\"Structured Pruning: Removing {prune_amount*100:.0f}% of filters from Conv layers...\")\n",
    "    \n",
    "    # Reset model to original state\n",
    "    model = copy.deepcopy(original_model)\n",
    "    \n",
    "    # Get all convolutional layers for structured pruning\n",
    "    conv_layers_to_prune = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            conv_layers_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    print(f\"  Found {len(conv_layers_to_prune)} convolutional layers to prune\")\n",
    "    \n",
    "    # Apply structured pruning to each conv layer individually\n",
    "    # We use L2 structured pruning which removes entire filters (channels)\n",
    "    for module, param_name in conv_layers_to_prune:\n",
    "        # Calculate number of filters to remove\n",
    "        num_filters = module.out_channels\n",
    "        num_to_remove = int(num_filters * prune_amount)\n",
    "        \n",
    "        if num_to_remove > 0:\n",
    "            # Apply structured pruning (removes entire filters)\n",
    "            prune.ln_structured(\n",
    "                module, \n",
    "                name=param_name, \n",
    "                amount=num_to_remove, \n",
    "                n=2,  # L2 norm\n",
    "                dim=0  # Remove along output channel dimension (entire filters)\n",
    "            )\n",
    "    \n",
    "    # Calculate structured sparsity (entire filters removed)\n",
    "    total_filters = 0\n",
    "    pruned_filters = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                # Count filters where all weights in the filter are zero\n",
    "                weight_mask = module.weight_mask\n",
    "                filter_sums = weight_mask.sum(dim=(1, 2, 3))  # Sum over spatial and input channel dims\n",
    "                total_filters += weight_mask.shape[0]\n",
    "                pruned_filters += (filter_sums == 0).sum().item()\n",
    "            else:\n",
    "                total_filters += module.out_channels\n",
    "    \n",
    "    structured_sparsity = pruned_filters / total_filters if total_filters > 0 else 0\n",
    "    \n",
    "    print(f\"  Structured sparsity achieved: {structured_sparsity*100:.2f}% (filters removed)\")\n",
    "    print(f\"  Performance after structured pruning:\")\n",
    "    \n",
    "    # Test pruned model\n",
    "    test_model(model, test_data, mode=mode)\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Store results\n",
    "    structured_results.append({\n",
    "        'target_prune': prune_amount * 100,\n",
    "        'structured_sparsity': structured_sparsity * 100\n",
    "    })\n",
    "\n",
    "print(\"\\nStructured Pruning Results Summary:\")\n",
    "print(\"Target Pruning % | Structured Sparsity % (Filters)\")\n",
    "print(\"-\" * 50)\n",
    "for result in structured_results:\n",
    "    print(f\"     {result['target_prune']:6.0f}%      |        {result['structured_sparsity']:6.2f}%\")\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h85dg7kVd_gk"
   },
   "source": [
    "## Inline Question 2:\n",
    "\n",
    "What is the difference between performing Structured Pruning vs Dropout ? \n",
    "Why would it be beneficial to perform both techniques when developing a Neural Network?\n",
    "\n",
    "\n",
    "## Answer: \n",
    "\n",
    "**Key Differences between Structured Pruning and Dropout:**\n",
    "\n",
    "1. **Timing and Purpose**:\n",
    "   - **Dropout**: Applied during training as a regularization technique to prevent overfitting\n",
    "   - **Structured Pruning**: Applied after training to reduce model size and computational requirements\n",
    "\n",
    "2. **Mechanism**:\n",
    "   - **Dropout**: Randomly sets neurons to zero during training (temporary, stochastic)\n",
    "   - **Structured Pruning**: Permanently removes entire structures (filters, channels, neurons) based on importance metrics\n",
    "\n",
    "3. **Permanence**:\n",
    "   - **Dropout**: Temporary - neurons are only disabled during training, all are active during inference\n",
    "   - **Structured Pruning**: Permanent - removed structures are gone forever, reducing model size\n",
    "\n",
    "4. **Impact on Architecture**:\n",
    "   - **Dropout**: No change to model architecture or size\n",
    "   - **Structured Pruning**: Actually changes model architecture and reduces parameters\n",
    "\n",
    "**Benefits of Using Both Techniques:**\n",
    "\n",
    "1. **Complementary Goals**: Dropout helps learn robust features during training, while pruning optimizes the final model for deployment\n",
    "\n",
    "2. **Better Generalization**: Dropout forces the network to not rely on specific neurons, creating redundancy that makes later pruning less harmful\n",
    "\n",
    "3. **Optimal Resource Usage**: Dropout helps identify which components are truly necessary, making pruning decisions more informed\n",
    "\n",
    "4. **Deployment Efficiency**: Training with dropout creates networks that are naturally more resilient to structural changes from pruning\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
